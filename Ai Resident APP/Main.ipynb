{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import UnstructuredURLLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain_community.vectorstores import Chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "OxyIe4RuCCRD"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "gmlgVsTOCCRE"
      },
      "outputs": [],
      "source": [
        "urls = [\n",
        "    'https://github.com/Hannibal046/Awesome-LLM?tab=readme-ov-file#milestone-papers',\n",
        "    'https://stanford-cs324.github.io/winter2022/lectures/',\n",
        "    'https://stanford-cs324.github.io/winter2022/lectures/introduction/',\n",
        "    'https://stanford-cs324.github.io/winter2022/lectures/capabilities/',\n",
        "    'https://stanford-cs324.github.io/winter2022/lectures/data/',\n",
        "    'https://stanford-cs324.github.io/winter2022/lectures/training/',\n",
        "    'https://stanford-cs324.github.io/winter2022/lectures/environment/',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "0U1XDi5SCCRF"
      },
      "outputs": [],
      "source": [
        "loaders = UnstructuredURLLoader(urls=urls)\n",
        "data = loaders.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "context = \"\\n\".join(str(p.page_content) for p in data)\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The total number of words in the context: 120248\n"
          ]
        }
      ],
      "source": [
        "print(\"The total number of words in the context:\", len(context))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=7000, chunk_overlap=2000)\n",
        "context = \"\\n\\n\".join(str(p.page_content) for p in data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "texts = text_splitter.split_text(context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Hannibal046\\n\\nAwesome-LLM\\n\\nPublic\\n\\nNotifications\\n    You must be signed in to change notification settings\\n\\nFork\\n    1.3k\\n\\nStar\\n          15.8k\\n\\nAwesome-LLM: a curated list of Large Language Model\\n\\nLicense\\n\\nCC0-1.0 license\\n\\n15.8k\\n          stars\\n\\n1.3k\\n          forks\\n\\nBranches\\n\\nTags\\n\\nActivity\\n\\nStar\\n\\nNotifications\\n\\nCode\\n\\nIssues\\n          0\\n\\nPull requests\\n          1\\n\\nActions\\n\\nProjects\\n          0\\n\\nSecurity\\n\\nInsights\\n\\nCode\\n\\nIssues\\n\\nPull requests\\n\\nActions\\n\\nProjects\\n\\nSecurity\\n\\nInsights\\n\\nHannibal046/Awesome-LLM\\n\\nThis commit does not belong to any branch on this repository, and may belong to a fork outside of the repository.\\n\\nmain\\n\\nBranches\\n\\nTags\\n\\nGo to file\\n\\nCode\\n\\nFolders and files\\n\\nName Name Last commit message Last commit date Latest commit History 433 Commits paper_list paper_list resources resources .gitignore .gitignore LICENSE.md LICENSE.md README.md README.md contributing.md contributing.md View all files\\n\\nRepository files navigation\\n\\nREADME\\n\\nCC0-1.0 license\\n\\nAwesome-LLM\\n\\n🔥 Large Language Models(LLM) have taken the NLP community AI community the Whole World by storm. Here is a curated list of papers about large language models, especially relating to ChatGPT. It also contains frameworks for LLM training, tools to deploy LLM, courses and tutorials about LLM and all publicly available LLM checkpoints and APIs.\\n\\nTrending LLM Projects\\n\\nmistral.rs - Blazingly fast LLM inference.\\n\\ndspy - DSPy: The framework for programming—not prompting—foundation models.\\n\\nQWen2 - Qwen2 is the large language model series developed by Qwen team, Alibaba Cloud.\\n\\nDeepSeek-Coder-V2 - an open-source Mixture-of-Experts (MoE) code language model that achieves performance comparable to GPT4-Turbo in code-specific tasks.\\n\\nTable of Content\\n\\nAwesome-LLM \\n\\nMilestone Papers\\nOther Papers\\nLLM Leaderboard\\nOpen LLM\\nLLM Data\\nLLM Evaluation\\nLLM Training Framework\\nLLM Deployment\\nLLM Applications\\nLLM Books\\nGreat thoughts about LLM\\nMiscellaneous\\n\\nMilestone Papers\\n\\nDate keywords Institute Paper Publication 2017-06 Transformers Google Attention Is All You Need NeurIPS 2018-06 GPT 1.0 OpenAI Improving Language Understanding by Generative Pre-Training 2018-10 BERT Google BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding NAACL 2019-02 GPT 2.0 OpenAI Language Models are Unsupervised Multitask Learners 2019-09 Megatron-LM NVIDIA Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism 2019-10 T5 Google Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer JMLR 2019-10 ZeRO Microsoft ZeRO: Memory Optimizations Toward Training Trillion Parameter Models SC 2020-01 Scaling Law OpenAI Scaling Laws for Neural Language Models 2020-05 GPT 3.0 OpenAI Language models are few-shot learners NeurIPS 2021-01 Switch Transformers Google Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity JMLR 2021-08 Codex OpenAI Evaluating Large Language Models Trained on Code 2021-08 Foundation Models Stanford On the Opportunities and Risks of Foundation Models 2021-09 FLAN Google Finetuned Language Models are Zero-Shot Learners ICLR 2021-10 T0 HuggingFace et al. Multitask Prompted Training Enables Zero-Shot Task Generalization ICLR 2021-12 GLaM Google GLaM: Efficient Scaling of Language Models with Mixture-of-Experts ICML 2021-12 WebGPT OpenAI WebGPT: Browser-assisted question-answering with human feedback 2021-12 Retro DeepMind Improving language models by retrieving from trillions of tokens ICML 2021-12 Gopher DeepMind Scaling Language Models: Methods, Analysis & Insights from Training Gopher 2022-01 COT Google Chain-of-Thought Prompting Elicits Reasoning in Large Language Models NeurIPS 2022-01 LaMDA Google LaMDA: Language Models for Dialog Applications 2022-01 Minerva Google Solving Quantitative Reasoning Problems with Language Models NeurIPS 2022-01 Megatron-Turing NLG Microsoft&NVIDIA Using Deep and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model 2022-03 InstructGPT OpenAI Training language models to follow instructions with human feedback 2022-04 PaLM Google PaLM: Scaling Language Modeling with Pathways 2022-04 Chinchilla DeepMind An empirical analysis of compute-optimal large language model training NeurIPS 2022-05 OPT Meta OPT: Open Pre-trained Transformer Language Models 2022-05 UL2 Google Unifying Language Learning Paradigms ICLR 2022-06 Emergent Abilities Google Emergent Abilities of Large Language Models TMLR 2022-06 BIG-bench Google Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models 2022-06 METALM Microsoft Language Models are General-Purpose Interfaces 2022-09 Sparrow DeepMind Improving alignment of dialogue agents via targeted human judgements 2022-10 Flan-T5/PaLM Google Scaling Instruction-Finetuned Language Models 2022-10 GLM-130B Tsinghua GLM-130B: An Open Bilingual Pre-trained Model ICLR 2022-11 HELM Stanford Holistic Evaluation of Language Models 2022-11 BLOOM BigScience BLOOM: A 176B-Parameter Open-Access Multilingual Language Model 2022-11 Galactica Meta Galactica: A Large Language Model for Science 2022-12 OPT-IML Meta OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization 2023-01 Flan 2022 Collection Google The Flan Collection: Designing Data and Methods for Effective Instruction Tuning ICML 2023-02 LLaMA Meta LLaMA: Open and Efficient Foundation Language Models 2023-02 Kosmos-1 Microsoft Language Is Not All You Need: Aligning Perception with Language Models 2023-03 PaLM-E Google PaLM-E: An Embodied Multimodal Language Model ICML 2023-03 GPT 4 OpenAI GPT-4 Technical Report 2023-04 Pythia EleutherAI et al. Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling ICML 2023-05 Dromedary CMU et al. Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision NeurIPS 2023-05 PaLM 2 Google PaLM 2 Technical Report 2023-05 RWKV Bo Peng RWKV: Reinventing RNNs for the Transformer Era EMNLP 2023-05 DPO Stanford Direct Preference Optimization: Your Language Model is Secretly a Reward Model Neurips 2023-05 ToT Google&Princeton Tree of Thoughts: Deliberate Problem Solving with Large Language Models NeurIPS 2023-07 LLaMA 2 Meta Llama 2: Open Foundation and Fine-Tuned Chat Models 2023-10 Mistral 7B Mistral Mistral 7B 2023-12 Mamba CMU&Princeton Mamba: Linear-Time Sequence Modeling with Selective State Spaces ICML 2024-03 Jamba AI21 Labs Jamba: A Hybrid Transformer-Mamba Language Model\\n\\nOther Papers\\n\\nIf you're interested in the field of LLM, you may find the above list of milestone papers helpful to explore its history and state-of-the-art. However, each direction of LLM offers a unique set of insights and contributions, which are essential to understanding the field as a whole. For a detailed list of papers in various subfields, please refer to the following link:\\n\\nAwesome-LLM-hallucination - LLM hallucination paper list.\""
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "24\n"
          ]
        }
      ],
      "source": [
        "print(len(texts))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "environ({'ALLUSERSPROFILE': 'C:\\\\ProgramData', 'APPDATA': 'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Roaming', 'APPLICATION_INSIGHTS_NO_DIAGNOSTIC_CHANNEL': 'true', 'BS4': ' c:\\\\users\\\\lenovo\\\\appdata\\\\local\\\\packages\\\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\\\localcache\\\\local-packages\\\\python39\\\\site-packages', 'C++': 'C:\\\\msys64\\\\mingw64\\\\bin', 'C++ COMPILER': 'C:\\\\msys64\\\\mingw64\\\\bin', 'CDS_HOME': 'C:\\\\cds_spb_home', 'CHOCOLATEYINSTALL': 'C:\\\\ProgramData\\\\chocolatey', 'CHOCOLATEYLASTPATHUPDATE': '133367473047977869', 'CHROME_CRASHPAD_PIPE_NAME': '\\\\\\\\.\\\\pipe\\\\crashpad_15184_ZJDLJFDWMHIOVSXU', 'COMMONPROGRAMFILES': 'C:\\\\Program Files\\\\Common Files', 'COMMONPROGRAMFILES(X86)': 'C:\\\\Program Files (x86)\\\\Common Files', 'COMMONPROGRAMW6432': 'C:\\\\Program Files\\\\Common Files', 'COMPUTERNAME': 'DEV-PC', 'COMSPEC': 'C:\\\\WINDOWS\\\\system32\\\\cmd.exe', 'CUDA_PATH': 'C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.1', 'CUDA_PATH_V12_1': 'C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.1', 'DRIVERDATA': 'C:\\\\Windows\\\\System32\\\\Drivers\\\\DriverData', 'EFC_11284': '1', 'ELECTRON_RUN_AS_NODE': '1', 'FPS_BROWSER_APP_PROFILE_STRING': 'Internet Explorer', 'FPS_BROWSER_USER_PROFILE_STRING': 'Default', 'HOMEDRIVE': 'C:', 'HOMEPATH': '\\\\Users\\\\Lenovo', 'IGCCSVC_DB': 'AQAAANCMnd8BFdERjHoAwE/Cl+sBAAAADVYex1NI+kaeFpWb8rz4BAQAAAACAAAAAAAQZgAAAAEAACAAAADIV+8luD6fIaHwwU8PqUlJUFdv3xc/Q9wHY577pX5j4wAAAAAOgAAAAAIAACAAAABBg8mT/e9WY2rpHmFJiktVsrFr0ccuGeaOUFZgeBI062AAAADcNZxDlHCam7MiA8mGpX4K6oyWcUboZGDUqkrzQSo4r3t4ntijmHw7AiOEDHp8M0eDkpxY3wPEDf6r33MW9cdz6tirUC7rME20Xpjhz8JeShtKa8UUl1i/lGQNIpKDUW1AAAAAqp09QFnMTfgEzU6drf0LDC9FW6qoMI8dM/TREJ3/IWeOIpwuKuo1mkeoCCwlPINLnpS7fuOfnUdfsfo3WxkanQ==', 'INTELLIJ IDEA COMMUNITY EDITION': 'C:\\\\Program Files\\\\JetBrains\\\\IntelliJ IDEA Community Edition 2022.1\\\\bin;', 'IVIROOTDIR32': 'C:\\\\Program Files (x86)\\\\IVI Foundation\\\\IVI\\\\', 'IVIROOTDIR64': 'C:\\\\Program Files\\\\IVI Foundation\\\\IVI\\\\', 'JD2_HOME': 'D:\\\\NFS', 'JPY_INTERRUPT_EVENT': '2428', 'LOCALAPPDATA': 'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local', 'LOGONSERVER': '\\\\\\\\DEV-PC', 'MYSQL': 'C:\\\\Program Files\\\\MySQL\\\\MySQL Server 8.0\\\\bin', 'NUMBER_OF_PROCESSORS': '8', 'ONEDRIVE': 'C:\\\\Users\\\\Lenovo\\\\OneDrive', 'OPENSSL_IA32CAP': '~0x200000200000000', 'ORIGINAL_XDG_CURRENT_DESKTOP': 'undefined', 'OS': 'Windows_NT', 'PATH': \"c:\\\\Users\\\\Lenovo\\\\Desktop\\\\Summer\\\\AIResident\\\\venv\\\\Scripts;C:\\\\Users\\\\Lenovo\\\\Desktop\\\\Summer\\\\AIResident\\\\venv\\\\Scripts;C:\\\\Program Files (x86)\\\\Common Files\\\\Oracle\\\\Java\\\\java8path;C:\\\\Program Files (x86)\\\\Common Files\\\\Oracle\\\\Java\\\\javapath;C:\\\\Program Files (x86)\\\\VMware\\\\VMware Player\\\\bin\\\\;C:\\\\Python311\\\\Scripts\\\\;C:\\\\Python311\\\\;C:\\\\Program Files\\\\Microsoft\\\\jdk-11.0.16.101-hotspot\\\\bin;C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.1\\\\bin;C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.1\\\\libnvvp;C:\\\\CUDNN\\\\cudnn-windows-x86_64-8.9.0.131_cuda11-archive\\\\lib\\\\x64;C:\\\\Windows\\\\system32;C:\\\\Windows;C:\\\\Windows\\\\System32\\\\Wbem;C:\\\\Windows\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\Windows\\\\System32\\\\OpenSSH\\\\;C:\\\\Program Files (x86)\\\\NVIDIA Corporation\\\\PhysX\\\\Common;C:\\\\Program Files\\\\NVIDIA Corporation\\\\NVIDIA NvDLISR;C:\\\\WINDOWS\\\\system32;C:\\\\WINDOWS;C:\\\\WINDOWS\\\\System32\\\\Wbem;C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\WINDOWS\\\\System32\\\\OpenSSH\\\\;C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps;C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Programs\\\\Microsoft VS Code\\\\bin;C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\Scripts;C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310;C:\\\\msys64\\\\mingw64\\\\bin;'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python39\\\\Scripts;c:\\\\users\\\\lenovo\\\\appdata\\\\local\\\\packa;C:\\\\MinGW\\\\bin;C:\\\\Program Files (x86)\\\\IVI Foundation\\\\VISA\\\\WinNT\\\\Bin\\\\;C:\\\\Program Files\\\\IVI Foundation\\\\VISA\\\\Win64\\\\Bin\\\\;C:\\\\Program Files (x86)\\\\IVI Foundation\\\\IVI\\\\Bin\\\\;C:\\\\Program Files\\\\IVI Foundation\\\\IVI\\\\Bin\\\\;C:\\\\Program Files (x86)\\\\IVI Foundation\\\\VISA\\\\WinNT\\\\Bin;D:\\\\Programmes\\\\Matlab 2014 New\\\\runtime\\\\win64;D:\\\\Programmes\\\\Matlab 2014 New\\\\bin;D:\\\\Programmes\\\\Matlab 2014 New\\\\polyspace\\\\bin;C:\\\\Program Files\\\\NVIDIA Corporation\\\\Nsight Compute 2023.1.1\\\\;C:\\\\CUDNN\\\\cudnn-windows-x86_64-8.9.0.131_cuda11-archive\\\\bin;C:\\\\CUDNN\\\\cudnn-windows-x86_64-8.9.0.131_cuda11-archive\\\\include;C:\\\\CUDNN\\\\cudnn-windows-x86_64-8.9.0.131_cuda11-archive\\\\lib;C:\\\\CUDNN\\\\cudnn-windows-x86_64-8.9.0.131_cuda11-archive;C:\\\\Program Files\\\\dotnet\\\\;C:\\\\Program Files\\\\nodejs\\\\;C:\\\\ProgramData\\\\chocolatey\\\\bin;C:\\\\Program Files\\\\Git\\\\cmd;C:\\\\Program Files\\\\MySQL\\\\MySQL Shell 8.0\\\\bin\\\\;C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps;C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Programs\\\\Microsoft VS Code\\\\bin;C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\Scripts;C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310;C:\\\\msys64\\\\mingw64\\\\bin;'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python39\\\\Scripts;c:\\\\users\\\\lenovo\\\\appdata\\\\local\\\\packages\\\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\\\localcache\\\\local-packages\\\\python39\\\\site-packages;C:\\\\Program Files\\\\JetBrains\\\\IntelliJ IDEA Community Edition 2022.1\\\\bin;;C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\GitHubDesktop\\\\bin;C:\\\\CUDNN\\\\cudnn-windows-x86_64-8.9.0.131_cuda11-archive\\\\lib;C:\\\\CUDNN\\\\cudnn-windows-x86_64-8.9.0.131_cuda11-archive\\\\include;C:\\\\CUDNN\\\\cudnn-windows-x86_64-8.9.0.131_cuda11-archive\\\\bin;C:\\\\Users\\\\Lenovo\\\\AppData\\\\Roaming\\\\npm\", 'PATHEXT': '.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC;.PY;.PYW', 'PROCESSOR_ARCHITECTURE': 'AMD64', 'PROCESSOR_IDENTIFIER': 'Intel64 Family 6 Model 140 Stepping 1, GenuineIntel', 'PROCESSOR_LEVEL': '6', 'PROCESSOR_REVISION': '8c01', 'PROGRAMDATA': 'C:\\\\ProgramData', 'PROGRAMFILES': 'C:\\\\Program Files', 'PROGRAMFILES(X86)': 'C:\\\\Program Files (x86)', 'PROGRAMW6432': 'C:\\\\Program Files', 'PROMPT': '(venv) $P$G', 'PSMODULEPATH': 'C:\\\\Program Files\\\\WindowsPowerShell\\\\Modules;C:\\\\WINDOWS\\\\system32\\\\WindowsPowerShell\\\\v1.0\\\\Modules', 'PSQL': 'C:\\\\Program Files\\\\PostgreSQL\\\\16\\\\bin', 'PUBLIC': 'C:\\\\Users\\\\Public', 'PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING': '1', 'PYTHONIOENCODING': 'utf-8', 'PYTHONUNBUFFERED': '1', 'PYTHONUSERBASE': 'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages', 'PYTHON_FROZEN_MODULES': 'on', 'SCRIPTS': \"'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python39\\\\Scripts\", 'SDL_AUDIODRIVER': 'directsound', 'SESSIONNAME': 'Console', 'SYSTEMDRIVE': 'C:', 'SYSTEMROOT': 'C:\\\\WINDOWS', 'TEMP': 'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Temp', 'TMP': 'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Temp', 'USERDOMAIN': 'DEV-PC', 'USERDOMAIN_ROAMINGPROFILE': 'DEV-PC', 'USERNAME': 'Lenovo', 'USERPROFILE': 'C:\\\\Users\\\\Lenovo', 'VBOX_MSI_INSTALL_PATH': 'C:\\\\Program Files\\\\Oracle\\\\VirtualBox\\\\', 'VIRTUAL_ENV': 'C:\\\\Users\\\\Lenovo\\\\Desktop\\\\Summer\\\\AIResident\\\\venv', 'VSCODE_AMD_ENTRYPOINT': 'vs/workbench/api/node/extensionHostProcess', 'VSCODE_CODE_CACHE_PATH': 'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Roaming\\\\Code\\\\CachedData\\\\5437499feb04f7a586f677b155b039bc2b3669eb', 'VSCODE_CRASH_REPORTER_PROCESS_TYPE': 'extensionHost', 'VSCODE_CWD': 'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Programs\\\\Microsoft VS Code', 'VSCODE_HANDLES_UNCAUGHT_ERRORS': 'true', 'VSCODE_IPC_HOOK': '\\\\\\\\.\\\\pipe\\\\bd59bc98-1.90.2-main-sock', 'VSCODE_L10N_BUNDLE_LOCATION': '', 'VSCODE_NLS_CONFIG': '{\"locale\":\"en-us\",\"osLocale\":\"en-in\",\"availableLanguages\":{},\"_languagePackSupport\":true}', 'VSCODE_PID': '15184', 'VXIPNPPATH': 'C:\\\\Program Files (x86)\\\\IVI Foundation\\\\VISA\\\\', 'VXIPNPPATH64': 'C:\\\\Program Files\\\\IVI Foundation\\\\VISA\\\\', 'WINDIR': 'C:\\\\WINDOWS', 'ZES_ENABLE_SYSMAN': '1', '_OLD_VIRTUAL_PATH': \"C:\\\\Program Files (x86)\\\\Common Files\\\\Oracle\\\\Java\\\\java8path;C:\\\\Program Files (x86)\\\\Common Files\\\\Oracle\\\\Java\\\\javapath;C:\\\\Program Files (x86)\\\\VMware\\\\VMware Player\\\\bin\\\\;C:\\\\Python311\\\\Scripts\\\\;C:\\\\Python311\\\\;C:\\\\Program Files\\\\Microsoft\\\\jdk-11.0.16.101-hotspot\\\\bin;C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.1\\\\bin;C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v12.1\\\\libnvvp;C:\\\\CUDNN\\\\cudnn-windows-x86_64-8.9.0.131_cuda11-archive\\\\lib\\\\x64;C:\\\\Windows\\\\system32;C:\\\\Windows;C:\\\\Windows\\\\System32\\\\Wbem;C:\\\\Windows\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\Windows\\\\System32\\\\OpenSSH\\\\;C:\\\\Program Files (x86)\\\\NVIDIA Corporation\\\\PhysX\\\\Common;C:\\\\Program Files\\\\NVIDIA Corporation\\\\NVIDIA NvDLISR;C:\\\\WINDOWS\\\\system32;C:\\\\WINDOWS;C:\\\\WINDOWS\\\\System32\\\\Wbem;C:\\\\WINDOWS\\\\System32\\\\WindowsPowerShell\\\\v1.0\\\\;C:\\\\WINDOWS\\\\System32\\\\OpenSSH\\\\;C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps;C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Programs\\\\Microsoft VS Code\\\\bin;C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\Scripts;C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310;C:\\\\msys64\\\\mingw64\\\\bin;'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python39\\\\Scripts;c:\\\\users\\\\lenovo\\\\appdata\\\\local\\\\packa;C:\\\\MinGW\\\\bin;C:\\\\Program Files (x86)\\\\IVI Foundation\\\\VISA\\\\WinNT\\\\Bin\\\\;C:\\\\Program Files\\\\IVI Foundation\\\\VISA\\\\Win64\\\\Bin\\\\;C:\\\\Program Files (x86)\\\\IVI Foundation\\\\IVI\\\\Bin\\\\;C:\\\\Program Files\\\\IVI Foundation\\\\IVI\\\\Bin\\\\;C:\\\\Program Files (x86)\\\\IVI Foundation\\\\VISA\\\\WinNT\\\\Bin;D:\\\\Programmes\\\\Matlab 2014 New\\\\runtime\\\\win64;D:\\\\Programmes\\\\Matlab 2014 New\\\\bin;D:\\\\Programmes\\\\Matlab 2014 New\\\\polyspace\\\\bin;C:\\\\Program Files\\\\NVIDIA Corporation\\\\Nsight Compute 2023.1.1\\\\;C:\\\\CUDNN\\\\cudnn-windows-x86_64-8.9.0.131_cuda11-archive\\\\bin;C:\\\\CUDNN\\\\cudnn-windows-x86_64-8.9.0.131_cuda11-archive\\\\include;C:\\\\CUDNN\\\\cudnn-windows-x86_64-8.9.0.131_cuda11-archive\\\\lib;C:\\\\CUDNN\\\\cudnn-windows-x86_64-8.9.0.131_cuda11-archive;C:\\\\Program Files\\\\dotnet\\\\;C:\\\\Program Files\\\\nodejs\\\\;C:\\\\ProgramData\\\\chocolatey\\\\bin;C:\\\\Program Files\\\\Git\\\\cmd;C:\\\\Program Files\\\\MySQL\\\\MySQL Shell 8.0\\\\bin\\\\;C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps;C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Programs\\\\Microsoft VS Code\\\\bin;C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\Scripts;C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310;C:\\\\msys64\\\\mingw64\\\\bin;'C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python39\\\\Scripts;c:\\\\users\\\\lenovo\\\\appdata\\\\local\\\\packages\\\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\\\localcache\\\\local-packages\\\\python39\\\\site-packages;C:\\\\Program Files\\\\JetBrains\\\\IntelliJ IDEA Community Edition 2022.1\\\\bin;;C:\\\\Users\\\\Lenovo\\\\AppData\\\\Local\\\\GitHubDesktop\\\\bin;C:\\\\CUDNN\\\\cudnn-windows-x86_64-8.9.0.131_cuda11-archive\\\\lib;C:\\\\CUDNN\\\\cudnn-windows-x86_64-8.9.0.131_cuda11-archive\\\\include;C:\\\\CUDNN\\\\cudnn-windows-x86_64-8.9.0.131_cuda11-archive\\\\bin;C:\\\\Users\\\\Lenovo\\\\AppData\\\\Roaming\\\\npm\", '_OLD_VIRTUAL_PROMPT': '$P$G', '__PSLOCKDOWNPOLICY': '0', '– OPENSSL_IA32CAP': '~0x200000200000000', 'PYDEVD_USE_FRAME_EVAL': 'NO', 'TERM': 'xterm-color', 'CLICOLOR': '1', 'FORCE_COLOR': '1', 'CLICOLOR_FORCE': '1', 'PAGER': 'cat', 'GIT_PAGER': 'cat', 'MPLBACKEND': 'module://matplotlib_inline.backend_inline', 'GOOGLE_API_KEY': 'AIzaSyBBY7zqAs94UgrnCQYyenm2CZet11SH0lg'})\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load the .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Print all environment variables for debugging\n",
        "print(os.environ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "vector_index = Chroma.from_texts(texts, embeddings).as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\n",
        "  Answer the question as detailed as possible from the provided context, make sure to provide all the details, if the answer is not in\n",
        "  provided context just say, \"answer is not available in the context\"\\n\\n\n",
        "  Context:\\n {context}?\\n\n",
        "  Question: \\n{question}\\n\n",
        "\n",
        "  Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template = prompt_template, input_variables = [\"context\", \"question\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",google_api_key=os.environ[\"GOOGLE_API_KEY\"]) \n",
        "\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(page_content='Benchmark data contamination.\\n\\nWhen we are evaluating the capabilities of large language models using benchmark data (e.g., question-answer pairs), it makes a difference whether the benchmark data appears in the training data of the language model. If so, then the benchmark performance will be biased up.\\n\\nNormally, in machine learning, data hygiene (keeping the training data separate from the test) is relatively easy, but in the case of large language models, both the training data and benchmark data are derived from the Internet, it can be difficult to a priori guarantee their separation.\\n\\nExample from the XSum summarization dataset:\\n\\nInput:\\n\\nThe 48-year-old former Arsenal goalkeeper played for the Royals for four years. He was appointed youth academy director in 2000 and has been director of football since 2003. A West Brom statement said: “He played a key role in the Championship club twice winning promotion to the Premier League in 2006 and 2012.\\n\\nOutput:\\n\\nWest Brom have appointed Nicky Hammond as technical director, ending his 20-year association with Reading.\\n\\nThere are two types of contamination:\\n\\nInput-and-output contamination: both the input and output appear in the training data. Varies from 1.87% to 24.88% (XSum is 15.49%).\\n\\nInput contamination: the input appears in the training data. Varies from 1.8% to 53.6% (QNLI, which is derived from Wikipedia).\\n\\nNote that contamination is not due to hosting datasets (as they are usually stored in a JSON file, not as a webpage).\\n\\nThe dataset could also be responsible for various harms:\\n\\nRepresentational harmsThey look at co-occurrence with ethnicity terms (e.g., Jewish) and sentiment-bearing words (e.g., successful).Jewish has 73.2% positive sentiment, Arab has 65.7% positive (7.5% difference).Variation across sites (New York Times had a 4.5% difference, Al Jazeera had 0% difference).\\n\\nAllocational harmsRecall C4 is a filtered version of Common Crawl (only about 10%).Mentions of sexual orientations (e.g., lesbian, gay) more likely to be filtered out; of those filtered out, non-trivial fraction are non-offensive (e.g., 22% and 36%).Certain dialects are more likely to be filtered (AAE: 42%, Hispanic-aligned English: 32%) than others (White American English: 6.2%)\\n\\nGPT-3 dataset\\n\\nSelected subset of Common Crawl that’s similar to a reference dataset (WebText).Downloaded 41 shards of Common Crawl (2016-2019).Trained a binary classifier to predict WebText versus Common Crawl.Sampled (kept) a document with higher probability if classifier deems it more similar to WebText.\\n\\nPerformed fuzzy deduplication (detect 13-gram overlap, remove window or documents if occurred in <10 training documents), removing data from benchmark datasets.\\n\\nExpanded the diversity of the data sources (WebText2, Books1, Books2, Wikipedia).\\n\\nDuring training, Common Crawl is downsampled (Common Crawl is 82% of the dataset, but contributes only 60%).\\n\\nThe Pile\\n\\nWhile a web crawl is a natural place to look for broad data, it’s not the only strategy, and GPT-3 already hinted that it might be productive to look at other sources of higher quality.\\n\\nEleutherAI (a nonprofit organization committed to building open language models), pushed this idea even farther.\\n\\nThey released The Pile, a dataset for language modeling, where the key idea is to source it from smaller high-quality sources (academic + professional sources).\\n\\nData composition.\\n\\n825 GB English text\\n\\n22 high-quality datasets\\n\\nCompare:\\n\\nGPT-2Pile (1.5B parameters) trained on The Pile\\n\\nGPT-3 (175B parameters) trained on GPT-3’s dataset.\\n\\nNormalize so that the difference for OpenWebText2 is 0.\\n\\nTakeaway: The Pile contains a lot of information that’s not well covered by GPT-3’s dataset.\\n\\nThey also performed analysis of pejorative content, gender/religion biases. The findings are qualitatively similar to previous work.\\n\\nSummary\\n\\nThe total amount of data out there (web, private data) is massive.\\n\\nTraining on “all of it” (even Common Crawl) doesn’t work well (not effective use of compute).\\n\\nFiltering / curation (OpenWebText, C4, GPT-3 dataset) is needed, but can result in biases.\\n\\nCurating non-web high-quality datasets is promising (The Pile).\\n\\nImportant to carefully document and inspect these datasets.\\n\\nDocumentation for datasets\\n\\nWe now step back from the specifics of language modeling datasets and discuss general principles around data.\\n\\nIt has been long noted that documentation is important, but within the machine learning community, it has been a fairly ad-hoc process.\\n\\nExamples from other fields:Electronics industry has a well-established protocol where every component has a datasheet with operating characteristics, test results, recommended and usage.Nutrition labels: The FDA mandates that food be labeled with their nutrition content.\\n\\nDatasheets for datasets (Gebru et al., 2018) is an influential paper that provides community norms around documentation.\\n\\nData statements (Bender & Friedman, 2018) is related framework that is more tailored to language datasets.\\n\\nThe emphasis is on transparency.\\n\\nTwo purposes:\\n\\nDataset creators: reflect on decisions, potential harms (e.g., social biases) when creating the dataset.\\n\\nDataset consumers: know when the dataset can and can’t be used.\\n\\nDataset lifecycle (a sample of the questions from each category are provided below):\\n\\nMotivationFor what purpose was the dataset created?Who created this dataset?Who funded the creation of the dataset?\\n\\nCompositionWhat do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?Is any information missing from individual instances?Does the dataset contain data that might be considered confidential?\\n\\nCollection processHow was the data associated with each instance acquired?Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?Were any ethical review processes conducted (e.g., by an institutional review board)?\\n\\nPreprocessing/cleaning/labelingWas any preprocessing/cleaning/labeling of the data done?Is the software that was used to preprocess/clean/label the data available?\\n\\nUsesHas the dataset been used for any tasks already?Are there tasks for which the dataset should not be used?\\n\\nDistributionHow will the dataset will be distributed?Have any third parties imposed IP-based or other restrictions on the data associated with the instances?\\n\\nMaintenanceWho will be supporting/hosting/maintaining the dataset?Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?\\n\\nData statements. The data statements work is specialized to NLP datasets, and covers other aspects:\\n\\nCuration rationale (what’s included?)\\n\\nLanguage variety (schema)\\n\\nSpeaker demographic (age, gender, race/ethnicity, etc.)\\n\\nAnnotator demographic (age, gender, race/ethnicity, etc.)\\n\\nAs an example, let’s look at the datasheet for The Pile.\\n\\nData ecosystems'), Document(page_content='Paper discussions\\n\\nProjects\\n\\nJust the Docs, a documentation theme for Jekyll.\\n\\nLectures\\n\\nData\\n\\nSo far, we’ve talked about the behavior (capabilities and harms) of large language models. Now, we peel open the first layer of the onion and start discussing how these models are constructed. The starting point of any machine learning approach is training data, so this is where we’ll start.\\n\\nAside: Normally in machine learning, the training data and the test (evaluation) data are similar or at least of the same type. But for large language models, the training data is just “raw text”.\\n\\nIn the rest of the lecture, we’ll talk about:\\n\\nData behind large language models\\n\\nDocumentation of datasets\\n\\nData ecosystems\\n\\nData behind large language models\\n\\nRecall that large language models are trained on “raw text”. To be highly capable (e.g., have linguistic and world knowledge), this text should span a broad range of domains, genres, languages, etc.\\n\\nA natural place (but not the only place) to look for such text is the web, so this will be a major focus of our attention. The web is absolutely huge. As a lower bound, the Google search index is 100 petabytes (reference). The actual web is likely even larger, and the Deep Web is even larger than that.\\n\\nIt is worth noting that private datasets that reside in big companies are even larger than what’s available publicly. For example, WalMart generates 2.5 petabytes of data each hour!\\n\\nCommon Crawl is a nonprofit organization that crawls the web and provides snapshots that are free to the public. Because of its convenience, it has been a standard source of data to train many models such as T5, GPT-3, and Gopher. The April 2021 snapshot of Common Crawl has 320 terabytes of data, which is a few orders of magnitude smaller than the Google index.\\n\\nRepresentation. Despite the richness of web data, it has been noted in Bender et al, 2021 that:\\n\\nDespite the size, large-scale data still has uneven representation over the population.\\n\\nInternet data overrepresents younger users from developed countries.\\n\\nGPT-2’s training data is based on Reddit, which according to Pew Internet Research’s 2016 survey, 67% of Reddit users in the US are men, 64% between ages 18 and 29.\\n\\n8.8-15% of Wikipedians are female.\\n\\nHarassment on Internet could turn away certain people (trans, queer, neurodivergent people).\\n\\nFiltering “bad words” could further marginalize certain populations (e.g., LGBT+).\\n\\nTakeaway: it is crucial to understand and document the composition of the datasets used to train large language models.\\n\\nWebText and OpenWebText\\n\\nWebText. The WebText dataset was used to train GPT-2.\\n\\nGoal: obtain diverse but high-quality dataset.\\n\\nPrevious work:Datasets were trained on news, Wikipedia, or fiction.Common Crawl contains a lot of junk (gibberish, boilerplate text).Trinh & Le, 2018 selected a tiny subset of Common Crawl based on n-gram overlap with the target task.\\n\\nProcess for creating WebText:Scraped all outbound links that received at least 3 karma (upvotes).Filtered out Wikipedia to be able to evaluate on Wikipedia-based benchmarks.End result is 40 GB of text.\\n\\nOpenWebText. WebText was not released by OpenAI, but it was replicated (in spirit) by the OpenWebText dataset.\\n\\nExtracted all the URLs from the Reddit submissions dataset.\\n\\nUsed Facebook’s fastText to filter out non-English.\\n\\nRemoved near duplicates.\\n\\nEnd result is 38 GB of text.\\n\\nToxicity analysis. Gehman et al. 2020, the RealToxicityPrompts paper, analyzed these two datasets and found:\\n\\n2.1% of OpenWebText has toxicity score >= 50%\\n\\n4.3% of WebText (from OpenAI) has toxicity score >= 50%\\n\\nNews reliability correlates negatively with toxicity (Spearman \\\\(\\\\rho = -0.35\\\\))\\n\\n3% of OpenWebText comes from banned or quarantined subreddits, e.g., /r/The_Donald and /r/WhiteRights\\n\\nColossal Clean Crawled Corpus\\n\\nThe Colossal Clean Crawled Corpus (C4) is a larger was created to train the T5 model.\\n\\nStarted with April 2019 snapshot of Common Crawl (1.4 trillion tokens)\\n\\nRemoved “bad words”\\n\\nRemoved code (“{“)\\n\\nlangdetect to filter out non-English text\\n\\nResulted in 806 GB of text (156 billion tokens)\\n\\nAnalysis. Dodge et al. 2021 performed a thorough analysis of the C4 dataset.\\n\\nDocumentation levels:\\n\\nMetadata: provenance, utterance data\\n\\nIncluded data: machine or human authored, social biases, data contamination\\n\\nExcluded data: medical or health data, demographic identities\\n\\nNote: Raffel et al. 2020 only provided scripts to recreate; cost thousands of dollars just to run these scripts.\\n\\nA surprising amount of data from patents.google.com\\n\\n65% pages in the Internet Archive; out of those, 92% pages written in the last decade\\n\\n51.3% pages are hosted in the United States; fewer from India even though lots of English speakers there\\n\\nSome text from patents.google.com are automatically created, and thus have systematic errors:Filed in a foreign country’s official language (e.g., Japanese) is automatically translated into EnglishAutomatically generated from optical character recognition (OCR)\\n\\nBenchmark data contamination.\\n\\nWhen we are evaluating the capabilities of large language models using benchmark data (e.g., question-answer pairs), it makes a difference whether the benchmark data appears in the training data of the language model. If so, then the benchmark performance will be biased up.\\n\\nNormally, in machine learning, data hygiene (keeping the training data separate from the test) is relatively easy, but in the case of large language models, both the training data and benchmark data are derived from the Internet, it can be difficult to a priori guarantee their separation.\\n\\nExample from the XSum summarization dataset:\\n\\nInput:\\n\\nThe 48-year-old former Arsenal goalkeeper played for the Royals for four years. He was appointed youth academy director in 2000 and has been director of football since 2003. A West Brom statement said: “He played a key role in the Championship club twice winning promotion to the Premier League in 2006 and 2012.\\n\\nOutput:\\n\\nWest Brom have appointed Nicky Hammond as technical director, ending his 20-year association with Reading.\\n\\nThere are two types of contamination:\\n\\nInput-and-output contamination: both the input and output appear in the training data. Varies from 1.87% to 24.88% (XSum is 15.49%).\\n\\nInput contamination: the input appears in the training data. Varies from 1.8% to 53.6% (QNLI, which is derived from Wikipedia).\\n\\nNote that contamination is not due to hosting datasets (as they are usually stored in a JSON file, not as a webpage).\\n\\nThe dataset could also be responsible for various harms:\\n\\nRepresentational harmsThey look at co-occurrence with ethnicity terms (e.g., Jewish) and sentiment-bearing words (e.g., successful).Jewish has 73.2% positive sentiment, Arab has 65.7% positive (7.5% difference).Variation across sites (New York Times had a 4.5% difference, Al Jazeera had 0% difference).'), Document(page_content='22 high-quality datasets\\n\\nCompare:\\n\\nGPT-2Pile (1.5B parameters) trained on The Pile\\n\\nGPT-3 (175B parameters) trained on GPT-3’s dataset.\\n\\nNormalize so that the difference for OpenWebText2 is 0.\\n\\nTakeaway: The Pile contains a lot of information that’s not well covered by GPT-3’s dataset.\\n\\nThey also performed analysis of pejorative content, gender/religion biases. The findings are qualitatively similar to previous work.\\n\\nSummary\\n\\nThe total amount of data out there (web, private data) is massive.\\n\\nTraining on “all of it” (even Common Crawl) doesn’t work well (not effective use of compute).\\n\\nFiltering / curation (OpenWebText, C4, GPT-3 dataset) is needed, but can result in biases.\\n\\nCurating non-web high-quality datasets is promising (The Pile).\\n\\nImportant to carefully document and inspect these datasets.\\n\\nDocumentation for datasets\\n\\nWe now step back from the specifics of language modeling datasets and discuss general principles around data.\\n\\nIt has been long noted that documentation is important, but within the machine learning community, it has been a fairly ad-hoc process.\\n\\nExamples from other fields:Electronics industry has a well-established protocol where every component has a datasheet with operating characteristics, test results, recommended and usage.Nutrition labels: The FDA mandates that food be labeled with their nutrition content.\\n\\nDatasheets for datasets (Gebru et al., 2018) is an influential paper that provides community norms around documentation.\\n\\nData statements (Bender & Friedman, 2018) is related framework that is more tailored to language datasets.\\n\\nThe emphasis is on transparency.\\n\\nTwo purposes:\\n\\nDataset creators: reflect on decisions, potential harms (e.g., social biases) when creating the dataset.\\n\\nDataset consumers: know when the dataset can and can’t be used.\\n\\nDataset lifecycle (a sample of the questions from each category are provided below):\\n\\nMotivationFor what purpose was the dataset created?Who created this dataset?Who funded the creation of the dataset?\\n\\nCompositionWhat do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)?Is any information missing from individual instances?Does the dataset contain data that might be considered confidential?\\n\\nCollection processHow was the data associated with each instance acquired?Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)?Were any ethical review processes conducted (e.g., by an institutional review board)?\\n\\nPreprocessing/cleaning/labelingWas any preprocessing/cleaning/labeling of the data done?Is the software that was used to preprocess/clean/label the data available?\\n\\nUsesHas the dataset been used for any tasks already?Are there tasks for which the dataset should not be used?'), Document(page_content='Read this article for more details.\\n\\nFurther reading\\n\\nDocumentation for datasets:\\n\\nDatasheets for datasets. Timnit Gebru, Jamie H. Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, H. Wallach, Hal Daumé, Kate Crawford. Communications of the ACM 2018.\\n\\nData Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science. Emily M. Bender and Batya Friedman. ACL 2018.\\n\\nModel Cards for Model Reporting. Margaret Mitchell, Simone Wu, Andrew Zaldivar, P. Barnes, Lucy Vasserman, B. Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru. FAT 2018.\\n\\nDatasets:\\n\\nCommonCrawl\\n\\nOpenWebText Similar to WebText, used to train GPT-2.\\n\\nExploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, W. Li, Peter J. Liu. J. Mach. Learn. Res. 2019. Introduces Clossal Clean Crawled Corpus (C4) and the T5 model.\\n\\nCCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data. Guillaume Wenzek, Marie-Anne Lachaux, A. Conneau, Vishrav Chaudhary, Francisco Guzm’an, Armand Joulin, Edouard Grave. LREC 2019. Introduces CCNet.\\n\\nThe Pile: An 800GB Dataset of Diverse Text for Language Modeling. Leo Gao, Stella Rose Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, Connor Leahy. 2020. Introduces The Pile. Introduces The Pile, used to train GPT-J.\\n\\nUnsupervised Cross-lingual Representation Learning at Scale. A. Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov. ACL 2019. Introduces cleaned versions of CommonCrawl corpus on 100 datasets, used to train XLM-R.\\n\\nAnalysis of datasets:\\n\\nDocumenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus. Jesse Dodge, Ana Marasović, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, Matt Gardner. EMNLP 2021.\\n\\nQuality at a Glance: An Audit of Web-Crawled Multilingual Datasets. Isaac Caswell, Julia Kreutzer, Lisa Wang, Ahsan Wahab, D. Esch, Nasanbayar Ulzii-Orshikh, A. Tapo, Nishant Subramani, A. Sokolov, Claytone Sikasote, Monang Setyawan, S. Sarin, Sokhar Samb, B. Sagot, Clara Rivera, Annette Rios Gonzales, Isabel Papadimitriou, Salomey Osei, Pedro Ortiz Suarez, Iroro Orife, Kelechi Ogueji, Rubungo Andre Niyongabo, Toan Q. Nguyen, Mathias Muller, A. Muller, S. Muhammad, N. Muhammad, Ayanda Mnyakeni, Jamshidbek Mirzakhalov, Tapiwanashe Matangira, Colin Leong, Nze Lawson, Sneha Kudugunta, Yacine Jernite, M. Jenny, Orhan Firat, Bonaventure F. P. Dossou, Sakhile Dlamini, N. D. Silva, Sakine cCabuk Balli, Stella Rose Biderman, A. Battisti, Ahmed Baruwa, Ankur Bapna, P. Baljekar, Israel Abebe Azime, A. Awokoya, Duygu Ataman, Orevaoghene Ahia, Oghenefego Ahia, Sweta Agrawal, Mofetoluwa Adeyemi. 2021.\\n\\nFiltering datasets:\\n\\nAn Empirical Exploration in Quality Filtering of Text Data. Leo Gao. 2021.\\n\\nDeduplicating Training Data Makes Language Models Better. Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, D. Eck, Chris Callison-Burch, Nicholas Carlini. 2021.\\n\\nData ecosystems:\\n\\nFoundation models report (data section)\\n\\nBigScience data governance working group\\n\\nData Shapley: Equitable Valuation of Data for Machine Learning. Amirata Ghorbani, James Y. Zou. ICML 2019.\\n\\nData Freedom Act\\n\\nCS324\\n\\nHome\\n\\nCalendar\\n\\nLecturesIntroductionCapabilitiesHarms IHarms IIDataSecurityLegalityModelingTrainingParallelismScaling lawsSelective architecturesAdaptationEnvironmental impact\\n\\nPaper reviews\\n\\nPaper discussions\\n\\nProjects\\n\\nJust the Docs, a documentation theme for Jekyll.\\n\\nLectures\\n\\nTraining\\n\\n\\\\(\\\\newcommand{\\\\sV}{\\\\mathcal{V}} \\\\newcommand{\\\\sO}{\\\\mathcal{O}} \\\\newcommand{\\\\sD}{\\\\mathcal{D}} \\\\newcommand{\\\\sN}{\\\\mathcal{N}} \\\\newcommand{\\\\R}{\\\\mathbb{R}} \\\\newcommand{\\\\E}{\\\\mathbb{E}} \\\\newcommand{\\\\x}{x_{1:L}} \\\\newcommand{\\\\tx}{\\\\tilde x_{1:L}} \\\\newcommand{\\\\nl}[1]{\\\\textsf{#1}} \\\\newcommand{\\\\softmax}{\\\\text{softmax}} \\\\newcommand{\\\\TransformerBlock}{\\\\text{TransformerBlock}} \\\\newcommand{\\\\EmbedTokenWithPosition}{\\\\text{EmbedTokenWithPosition}} \\\\newcommand{\\\\SentenceEmbedding}{\\\\text{SentenceEmbedding}} \\\\newcommand{\\\\BERT}{\\\\text{BERT}} \\\\newcommand{\\\\MASK}{\\\\nl{[MASK]}} \\\\newcommand{\\\\SEP}{\\\\nl{[SEP]}} \\\\newcommand{\\\\CLS}{\\\\nl{[CLS]}} \\\\newcommand{\\\\generate}[1]{\\\\stackrel{#1}{\\\\rightsquigarrow}} \\\\newcommand{\\\\embed}{\\\\stackrel{\\\\phi}{\\\\Rightarrow}}\\\\) Last lecture, we talked about the model architecture for large language models (e.g., the Transformer). In this lecture, we will discuss how to train large language models.\\n\\nObjective functions\\n\\nOptimization algorithms\\n\\nObjective functions\\n\\nWe will consider objective functions for the three types of language models:\\n\\nDecoder-only (e.g., GPT-3): compute unidirectional contextual embeddings, generate one token at a time\\n\\nEncoder-only (e.g., BERT): compute bidirectional contextual embeddings\\n\\nEncoder-decoder (e.g., T5): encode input, decode output\\n\\nWe can use any model that maps token sequences into contextual embeddings (e.g., LSTMs, Transformers):\\\\[\\\\phi : \\\\sV^L \\\\to \\\\R^{d \\\\times L}.\\\\] \\\\[[\\\\nl{the}, \\\\nl{mouse}, \\\\nl{ate}, \\\\nl{the}, \\\\nl{cheese}] \\\\embed \\\\left[\\\\binom{1}{0.1}, \\\\binom{0}{1}, \\\\binom{1}{1}, \\\\binom{1}{-0.1}, \\\\binom{0}{-1} \\\\right].\\\\]\\n\\nDecoder-only models\\n\\nRecall that an autoregressive language model defines a conditional distribution:\\\\[p(x_i \\\\mid x_{1:i-1}).\\\\]\\n\\nWe define it as follows:\\n\\nMap \\\\(x_{1:i-1}\\\\) to contextual embeddings \\\\(\\\\phi(x_{1:i-1})\\\\).\\n\\nApply an embedding matrix \\\\(E \\\\in \\\\R^{V \\\\times d}\\\\) to obtain scores for each token \\\\(E \\\\phi(x_{1:i-1})_{i-1}\\\\).\\n\\nExponentiate and normalize it to produce the distribution over \\\\(x_i\\\\).\\n\\nSuccinctly:\\\\[p(x_{i+1} \\\\mid x_{1:i}) = \\\\softmax(E \\\\phi(x_{1:i})_i).\\\\]\\n\\nMaximum likelihood. Let \\\\(\\\\theta\\\\) be all the parameters of large language models.')]\n"
          ]
        }
      ],
      "source": [
        "# question = \"What are some milestone model architectures and papers in the last few years\"\n",
        "# question = \"What are the layers in a transformer block\"\n",
        "# question = \"What are trending llms\"\n",
        "question= \"Tell me about datasets used to train LLMs and how they’re cleaned\"\n",
        "docs = vector_index.get_relevant_documents(question)\n",
        "print(docs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datasets used to train LLMs are derived from the Internet, which makes it difficult to guarantee their separation from benchmark data. \n",
            "\n",
            "To address this issue, various filtering and curation techniques are employed:\n",
            "\n",
            "**Input-and-output contamination:** Both the input and output appear in the training data. Varies from 1.87% to 24.88% (XSum is 15.49%).\n",
            "\n",
            "**Input contamination:** The input appears in the training data. Varies from 1.8% to 53.6% (QNLI, which is derived from Wikipedia).\n",
            "\n",
            "**WebText**. The WebText dataset was used to train GPT-2. It was created by scraping all outbound links that received at least 3 karma (upvotes) and filtering out Wikipedia.\n",
            "\n",
            "**OpenWebText**. OpenWebText is a replication of WebText in spirit. It was created by extracting all the URLs from the Reddit submissions dataset, using Facebook's fastText to filter out non-English text, and removing near duplicates.\n",
            "\n",
            "**Colossal Clean Crawled Corpus (C4)**. C4 is a larger dataset that was created to train the T5 model. It was started with the April 2019 snapshot of Common Crawl (1.4 trillion tokens), and then removed \"bad words,\" code, and non-English text.\n",
            "\n",
            "**The Pile**. The Pile is a non-web dataset that was created to train the GPT-J model. It is a collection of 825 GB of English text from 22 high-quality datasets.\n"
          ]
        }
      ],
      "source": [
        "response = chain.invoke(\n",
        "    {\"input_documents\":docs, \"question\": question}\n",
        "    , return_only_outputs=True)\n",
        "print(response['output_text'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
